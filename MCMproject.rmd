---
title: 'Компьютерная работа №2'
author: "Группа 6: Королева Яна, Заруцкий Михаил, Лесных Мария, Ольховский Феликс"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r message = FALSE, warning = FALSE, echo = FALSE}
library(knitr)
library(kableExtra)
library(magrittr)
library(openxlsx)
library(pander)
library(corrplot)
library(caret)
library(FactoMineR)
library(factoextra)
library(devtools)
library(rio)
library(corrplot)
library(psych)
library(ggpubr)
library(REdaS)
```

### Recap

Продолжаем исследовать ценообразование на рынке недвижимости напримере данных штате Айова, США. Итак, в первой части мы предположили, что на цену продажи дома могут влиять следующие факторы: жилая площадь, площадь подвала, гаража, крытого крыльца, год постройки и продажи, а также принадлежность району. Загрузим заново данные и проведем аналогичные предыдущей части проекта разования:
```{r}
housing <- read.csv("housing.csv")
housing <- dplyr::mutate_if(housing, is.character, as.factor)
housing = housing %>% dplyr::select(SalePrice, LotArea, YearBuilt, YrSold, GrLivArea, TotalBsmtSF, GarageArea, EnclosedPorch, Neighborhood)

out_of_1.5IQR <- boxplot.stats(housing$SalePrice)$out
housing <- housing %>% filter(housing$SalePrice < min(out_of_1.5IQR))
```

Повторное описание данных:

```{r}
desc <- read.xlsx('housing_var.xlsx', sheet = 'Description_of_variables')
kbl(desc, caption = "Таблица 2. Описание отобранных переменных", booktabs = T, 
    col.names = c("Переменная", "Описание переменной, англ")) %>% 
  kable_classic_2(html_font = "Cambria", font_size = 10, full_width = F) %>%
  pack_rows("Зависимая переменная", 1, 1) %>%
  pack_rows("Объясняющая переменная", 2, 8) %>%
  pack_rows("Прочее", 9, 9)
```


### Выделение главных компонент


Перед тем как применить МГК, нужно первично проанализировать данные и проверить обоснованность его использования. Удалим целевую и нечисловые переменные из исходного датасета и посмотрим на распределение переменных с помощью ящичковых диаграмм:
```{r}
df1 <- housing[, 2:8]

boxplot(df1, xaxt = "n", cex.lab = 0.7)
text(x =  seq_along(names(df1)), y = par("usr")[3] - 1, srt = 35, adj = 1,
     labels = names(df1), xpd = TRUE, cex = 0.6)
```


График оказался нечитаем из-за неравномерного распределения -- слишком много выбросов у всех переменных. Применим простейшую нормировку данных, это приближает эмпирическое распределение к нормальному, что позволяет использовать тесты для проверки адекватности выборки применению методов компонентного анализа. Посмотрим, что изменится:
```{r}
df1_new <- scale(df1)

boxplot(df1_new, xaxt = "n", cex.lab = 0.7)
text(x =  seq_along(names(df1_new)), y = par("usr")[3] - 1, srt = 35, adj = 1,
     labels = names(df1_new), xpd = TRUE, cex = 0.6)
```

Действительно, благодаря нормированию средние находятся на одном уровне, а интерквартильные размахи примерно равны, поэтому мы можем работать с новым датасетом

\
Теперь построим корреляционную матрицу по отнормированным данным:
```{r}
corrplot(cor(df1_new), type = "full", method = "circle", tl.col = "black", col = COL2('PuOr', 10), tl.srt = 45, tl.cex = 0.5)
```

ВЫВОДЫЫЫЫЫЫЫ

\
Снижение размерности будет достигаться, если признаки, описывающие совокупность, достаточно коррелированы между собой. Применим тест сферичности `Бартлетта`, который используется для оценки эффективности методов снижения размерности. Он проверяет нулевую гипотезу о том, что теоретическая корреляционная матрица многомерного распределения вектора случайных величин является единичной матрицей. Если нулевая гипотеза отвергается, предпосылки применения метода главных компонента будут выполнены.
```{r}
bart_spher(df1_new)
```

Полученное значение `p-value` очень мало и составляет $< 2.22e-16$, что точно меньше уровня значимости в $5\%$, поэтому мы можем отвергнуть нулевую гипотезу, предпосылки применения метода главных компонента будут выполнены.


\
Теперь нужно определить оптимальное количество главных компонент, которые необходимо оставить для дальнейшего анализа. Попробуем это сделать тремя разными способами.

> Метод Кайзера

Этот метод предполагает отбор тех главных компонент, которым соответствует собственное значение $> 1$
```{r message=FALSE, warning=FALSE}
pca <- PCA(df1_new, graph = FALSE)
pander(pca$eig)

plot(eigen(cor(df1_new))$values, bstick = TRUE, type = 'b', main = '', xlab = 'Номер компоненты', 
     ylab = 'Собственное значение')
abline(h = 1, col = 'darkorchid4', lwd = 2)
text(x = 6, y = 1.1, 'eigen value = 1', col = 'darkorchid4')
```

Из таблички видно, что параметр `eigenvalue` превышает единицу только в первых двух компонентах, однако график определяет оптимальным число $3$ (собственное значение очень близко к единице и составляет $0.9962$).

> Критерий каменистой осыпи

На основе анализа "графика каменистой осыпи" мы должны будем выбрать такое число компонент, после которого происходит резкое падение доли сохраненной дисперсии:

```{r}
fviz_eig(pca, addlabels = TRUE, barfill="white", barcolor ="black",
               linecolor ='darkorchid4')
```

Судя по графику, мы должны отобрать $2$ компоненты (резкое падение только от $1$ до $2$).

> Доля суммарной вариации

Теперь воспользуемся другим методом -- определим оптимальное число в соответствии с долей суммарной вариации исходных признаков, которую требуется сохранить.

```{r message=FALSE, warning=FALSE}
princomp <- princomp(df1_new, cor = TRUE) 

cumvarsum <- cumsum(princomp$sdev^2 / sum(princomp$sdev^2))*100
plot(cumvarsum, bstick = TRUE, type = 'b', main = '', xlab = 'Номер компоненты', 
     ylab = 'Кумулятивное значение вариации, %')

abline(h = 70, col = 'darkorchid4', lwd = 2)
abline(h = 80, col = 'darkgoldenrod2', lwd = 2)
text(x = 7, y = 75, '70%', col = 'darkorchid4')
text(x = 7, y = 85, '80%', col = 'darkgoldenrod2')
``` 

В промежуток от $70\%$ до $80\%$ (именно такую долю обычно требуется сохранить) вошло одно значение -- $4$, именно такое число главных компонент будет обеспечивать необходимую долю суммарной вариации.

\
Итак, финально мы решили, что оптимальное число ГК -- $3$, так как $2$ по критерию каменистой осыпи и таблице с собственными значчениями не обеспечут необходимую долю суммарной вариации, а $4$ было бы слишком большим значением при существовании всего $7$ переменных. Суммарный вклад первых главных компонент можно увидеть в столбце `cumulative percentage of variance` в таблице выше: три отобранные ГК объясняют $65.4\%$ вариации. График накопленного вклада главных компонент в суммарную дисперсию исходного признакового пространства можно увидеть чуть выше.


\
Перейдем к матрице факторных нагрузок. Мы предполагаем $3$ сферы влияния на зависимую переменную.
```{r message=FALSE, warning=FALSE}
pc <- principal(cor(df1_new), nfactors = 3, rotate = "none", covar = FALSE)

pc_loadings <- matrix(as.numeric(pc$loadings), ncol = 3, nrow = ncol(df1_new))
rownames(pc_loadings) <- colnames(df1_new)

kbl(round(pc_loadings, 3),
    caption = "Таблица 2. Матрица факторных нагрузок", 
    booktabs = T, col.names = c("PC1", "PC2", "PC3")) %>% 
    kable_classic(html_font = "Cambria", font_size = 12, full_width = F)
```

Из этой таблицы можно понять, к какой ГК относится каждая из переменных. Нужно смотреть на то, где значение больше: к `PC1` относятся `YearBuilt`, `GrLivArea`, `TotalBsmtSF` и `GarageArea`, к `PC2` `LotArea` и `EnclosedPorch`, а к `PC3` -- только `YrSold`. Первую главную компоненту можно назвать "характеристики постройки", так как именно площади разных частей дома и год постройки определяют  качество и вместимость жилища. Второй главной компоненте можно дать название "характеристики участка", потому что площадь всего участка и крытого крыльца все же относятся не к основной постройке, а вот третья ГК состоит всего из одной переменной, поэтому и название соответствующее -- "год продажи"

### Построение уравнения регрессии с использованием выделенных ГК

Теперь перейдем к построению регрессии на $3$ главные компоненты:
```{r}
df_reg <- housing[, 1:8]
df_reg_sc <- scale(df_reg)

norm_df <- as.data.frame(df_reg_sc)

pc <- PCA(norm_df, graph = FALSE)
pcdf <- as.data.frame(pc$ind$coord[, 1:3])
pc1 <- pcdf$Dim.1
pc2 <- pcdf$Dim.2
pc3 <- pcdf$Dim.3

lm_pc <- lm(norm_df$SalePrice ~ pc1 + pc2 + pc3)
pander(summary(lm_pc))
```

Что мы получили: в построенной регрессионной модели на $3$ главные компоненты все коэффициенты значимы на уровне $5\%$ (`p-value` у первых двух ГК равен/практически равен нулю, а у `pc3` составляет $0.047$, что тоже меньше уровня значимости). Объясняющая точность также достаточно высока -- значение $R^2$ составляет целых $0.8077$, что говорит о $81\%$ доле объясненной дисперсии

\
Теперь сравним результаты с найденными ранее оптимальными регрессионными моделями. Выведем заново линейную модель множественной регрессии `lm3` по отобранным переменным со значимыми коэффициентами из предыдущей части проекта:
```{r}
lm3 <- lm(SalePrice ~ LotArea + YearBuilt + GrLivArea + TotalBsmtSF + GarageArea, df_reg)
pander(summary(lm3))
```

Также выведем оптимальную экспоненциальную модель `lm4`, кстати, относительное качество нелинейной регрессионной модели было выше
```{r}
lm4 <- lm(log(SalePrice) ~ LotArea + YearBuilt + GrLivArea + TotalBsmtSF + GarageArea, df_reg)
pred4 <- predict(lm4)
pander(summary(lm4))
```

Сравним полученные результаты: регрессионная модель, построенная по МГК, действительно лучше, чем обе модели из первой части проекта (отбор регрессоров при помощи метода включения и экспоненциальная модель). Во-первых, объясняющая способность модели `lm_pc` выросла на $10\%$ (сравниваем по скорректированному $R^2$ из-за разного количества объясняющих переменных) -- у `lm_3` она составляла $0.7034$, у `lm_4` -- $0.7071$, а после использования МГК стала равна $0.8073$. Во-вторых, существенно уменьшилась стандартная остаточная ошибка. По моделям из прошлого проекта `RSE` составляет $32256$ и $0.1925$ соответственно, а после применения МГК стала составлять $0.439$. Значение стандартной остаточной ошибки является хорошим критерием для оценки регрессионной модели: по сравнению с линейной моделью `lm_3` использование МГК помогло сильно сократить этот параметр, но значение все же стало выше, чем у модели `lm_4`. Мы посчитали, что возросшая на $10\%$ доля объясненной дисперсии компенсирует небольшое повышение `RSE`, поэтому уравнение регрессии на ГК является оптимальным.


### Кластерный анализ

Так как признаки достаточно сильно коррелируемы, то будем использовать *расстояние Махаланобиса*. Рассмотрим матрицу расстоний:
```{r}
mahalnobis <- sqrt(D2.dist(df1, cov(df1)))
```


Рассмотрим различные принципы определения расстояния между кластерами.

```{r}
fviz_dend(hclust, cex = 0.5,  k = 4, color_labels_by_k = TRUE, 
          main = 'Дендрограмма, расстояние Махаланобиса (принцип Варда)', ylab = 'Расстояние')
```

> Метод Варда

Этот метод дает наиболее удачное разбиение. Его принцип основан на минимизации суммы внутрикластерных дисперсий
```{r}
hclust_w <- hclust(mahalnobis, method = 'ward.D2')
```

> Метод ближнего соседа

Идейно дистанция между двумя кластерами определяется как расстояние между парой наблюдений, расположенных друг к другу ближе всего, причём каждое наблюдение берётся из своего кластера
```{r}
hclust_nn <- hclust(mahalnobis, method = 'single')
```

> Метод дальнего соседа


```{r}
hclust_fn <- hclust(mahalnobis, method = 'complete')
```

> Метод средней связи

```{r}
hclust_av <- hclust(mahalnobis, method = 'average')
```

> Метод центра тяжести

```{r}
hclust_c <- hclust(mahalnobis, method = 'centroid')
```

\
Теперь изобразим результаты иерархической кластеризации с помощью дендрограммы:
```{r}
hclust_w$labels <- df1$SalePrice

fviz_dend(hclust_w, cex = 0.5,  k = 4, color_labels_by_k = TRUE, k_colors = c('darkgoldenrod4', 'gold1','plum1', 'darkorchid4'),
          main = 'Дендрограмма, расстояние Махаланобиса (принцип Варда)', ylab = 'Расстояние')
```

Данная дендрограмма построена по методу Вапда, именно она выдаёт очень хорошую кластеризацию, в то время как остальные, полученные другими способами (смотреть в приложении) выдают очень плохое разбиение.

\
Таким образом, получилось $4$ кластера (на выведенной дендрограмме представлены $4$ различных цвета:коричневый, желтый, розовый и фиолетовый).


### Использование метода к-средних для классификации объектов


Перейдем к методу k-means, который должен лучше работать на больших выборках. Работаем все с теми же переменными из `cluster_df` с оптимальным количеством кластеров $k=2$
```{r}
kmeans4 <- kmeans(df1_new, centers = 4)
kmeans4$centers
```

Уже можно отметить различия в средних значениях по двум кластерам: например, среднее по взрослой смертности в первой группе составляет $0.621$, а по второй $-0.396$. Для удобства сразу перейдем к визуализации результатов через график средних
```{r}
plot(1:ncol(df1_new), kmeans4$centers[1,], xaxt = 'n', type = 'l', col = 'violetred2', 
     cex.sub = 0.3, cex.lab = 0.8, lwd = 2, ylim = c(-2, 5), 
     ylab = 'Среднее значение признака', xlab = 'Классифицирующий признак')

lines(1:ncol(df1_new), kmeans4$centers[2,], type = 'l', col = 'forestgreen', lwd = 2)
lines(1:ncol(df1_new), kmeans4$centers[3,], type = 'l', col = 'darkorchid4', lwd = 2)
lines(1:ncol(df1_new), kmeans4$centers[4,], type = 'l', col = 'gold1', lwd = 2)

title('График средних (признаки стандартизованы)')
legend(0.8, 5, c('Кластер 1', 'Кластер 2', 'Кластер 3', 'Кластер 4'),
       lwd = c(2, 2), col = c('violetred2', 'forestgreen', 'darkorchid4', 'gold1'))
text(x =  seq_along(names(df1_new)), y = par("usr")[3] - 1, srt = 15, adj = 0.5,
     labels = names(df1_new), xpd = TRUE, cex = 0.5)
```
